{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Homework 2\n",
    "## Regularization, Logistic Regression\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Summer 2019**<br/>\n",
    "**Instructor**: Pavlos Protopapas <br>\n",
    "**Homework prepared by:** David Sondak and Andrea Porelli\n",
    "\n",
    "---\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "- Submisson instructions:\n",
    "  - Submit your Jupyter notebook to the teaching staff in an email.\n",
    "  - The email should have the following subject:  gec-summer-2019 HW 2 \"Your Name\"\n",
    "    - e.g. gec-summer2019 HW2 David Sondak\n",
    "    \n",
    "    **Homeworks with the wrong subject line will recieve a 0.**\n",
    "  - The due date is Friday, August 23rd at 23:59 PM EDT.  Please note the time-zone!\n",
    "    ** No late days!**\n",
    "\n",
    "- Restart the kernel and run the whole notebook again before you submit. \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 [25 pts]: Regularization\n",
    "## Problem Description:  Forecasting Bike Sharing Usage\n",
    "Part 1 of this homework (HW2) continues Part 2 of HW1.  You will continue to build regression models for the Capital Bikeshare program in Washington D.C.  Part 2 of HW1 contains a detailed description of the dataset in case you need a reminder.\n",
    "\n",
    "**Important:** The present problem (Part 1 of HW2) assumes that you have already pre-processed the dataset.  If you did not save the processed data from subparts $2.1$ and $2.2$ in HW1, then you must redo the pre-processing.  Specifically>\n",
    "* You must convert categorical attributes into multiple binary attributes using one-hot encoding.\n",
    "* Scale each continuous predictor to have zero mean and a standard deviation of  $1$.\n",
    "See subpart $2.2$ in HW1 for even more details.\n",
    "\n",
    "##### Goals of this Part\n",
    "You will fit a multilinear regression model using using lasso and ridge regression and compare their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.1 [25 pts]:  Ridge and Lasso Regression\n",
    "\n",
    "### Part 1.1.1 [6 pts]:  Fit Linear, Ridge, and Lasso Regression Models\n",
    "Use the following regression techniques to fit linear models to the training set:\n",
    "- Ridge regression\n",
    "- Lasso regression\n",
    "\n",
    "Choose the regularization parameter $\\lambda$ from the set $\\{0.001, 0.005,..., 50, 100\\}$ using cross-validation.\n",
    "\n",
    "#### Hints\n",
    "- You may use `sklearn`'s `LinearRegression`, `RidgeCV` and `LassoCV` classes to implement Linear, Ridge, and Lasso regression. These classes automatically perform cross-validation to tune the parameter $\\lambda$ from a given range of values.\n",
    "- You may use the `plt.errorbar` function to plot confidence bars for the average $R^2$ scores.\n",
    "\n",
    "#### Deliverables\n",
    "Your code should be contained in a Jupyter notebook cell.  An appropriate level of comments is necessary.  Your code should run and output the required outputs described below.\n",
    "\n",
    "#### Required Outputs\n",
    "- Print the train and test $R^2$ scores for each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 [4 pts]:  Discuss your results by answering the following questions:\n",
    "1. [1 pt] How do the estimated coefficients compare to or differ from the coefficients estimated by a plain linear regression (without regularzation)?\n",
    "\n",
    "  *Your answer here.*\n",
    "\n",
    "2. [1 pt] Is there a difference between coefficients estimated by the two shrinkage methods? If so, give an explantion for the difference.\n",
    "\n",
    "  *Your answer here.*\n",
    "\n",
    "3. [1 pt] List the predictors that are assigned a coefficient value close to 0 (say < 1e-10) by the two methods.\n",
    "\n",
    "  *Your answer here.*\n",
    "\n",
    "4. [1 pt] Is there a difference in the way Ridge and Lasso regression assign coefficients to the predictors `temp` and `atemp`? If so, explain the reason for the difference.\n",
    "\n",
    "  *Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 [12 pts]:  Analyze the performance of the two regularization methods for different training sample sizes\n",
    "It would be nice to assess the performance of the regularization methods to figure out which one works better.  To do this, you will execute the following steps:\n",
    "- Generate random samples of sizes 100, 150, ..., 400 from the training set.\n",
    "- Do a random split of the data at each sample size.  You will do this 10 times.\n",
    "- Fit linear, Ridge and Lasso models on each training set.\n",
    "- Calculate the mean $R^2$ score and it's standard deviation (SD) for each model.\n",
    "- Compare the performance.\n",
    "\n",
    "#### Hints\n",
    "- You may use the following code to draw a random sample of a specified size from the training set:\n",
    "\n",
    "```python\n",
    "#--------  sample\n",
    "# A function to select a random sample of size k from the training set\n",
    "# Input: \n",
    "#      x (n x d array of predictors in training data)\n",
    "#      y (n x 1 array of response variable vals in training data)\n",
    "#      k (size of sample) \n",
    "# Return: \n",
    "#      chosen sample of predictors and responses\n",
    "\n",
    "def sample(x, y, k):\n",
    "    n = x.shape[0] # No. of training points\n",
    "    \n",
    "    # Choose random indices of size 'k'\n",
    "    subset_ind = np.random.choice(np.arange(n), k.astype(int))\n",
    "    \n",
    "    # Get predictors and reponses with the indices\n",
    "    x_subset = x[subset_ind, :]\n",
    "    y_subset = y[subset_ind]\n",
    "    \n",
    "    return (x_subset, y_subset)\n",
    "```\n",
    "\n",
    "#### Deliverables\n",
    "Your code should be contained in a Jupyter notebook cell.  An appropriate level of comments is necessary.  Your code should run and output the required outputs described below.\n",
    "\n",
    "#### Required Outputs\n",
    "- Fit Linear, Ridge and Lasso regression models to each of the generated samples. \n",
    "  - In each case, compute the $R^2$ score for the model on the training sample on which it was fitted and on the test set.\n",
    "- Repeat the above experiment for $10$ random trials\n",
    "  - Compute the average train and test $R^2$ across the trials for each training sample size.\n",
    "  - Compute the standard deviation (SD) in each case.\n",
    "- Make a plot of the mean training $R^2$ scores for the Linear, Ridge and Lasso regression methods as a function of the training sample size.\n",
    "- Show a confidence interval for the mean scores extending from **mean - SD** to **mean + SD**.\n",
    "- Make a similar plot for the test $R^2$ scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 [3 pts]:  Discuss your results by answering the following questions:\n",
    "1. [1 pt] How do the training and test $R^2$ scores compare for the three methods? Give an explanation for your observations.\n",
    "\n",
    "  *Your answer here.*\n",
    "\n",
    "2. [1 pt] How do the confidence intervals for the estimated $R^2$ change with training sample size?\n",
    "\n",
    "  *Your answer here.*\n",
    "\n",
    "3. [1 pt] Based on the plots, which of the three methods would you recommend when one needs to fit a regression model using a small training sample?\n",
    "\n",
    "  *Your answer here.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 [22 pts]:  Logistic Regression\n",
    "\n",
    "## Problem Description:  Cancer Classification from Gene Expressions\n",
    "\n",
    "In this part of the assignment, you will build a classification model to distinguish between two related classes of cancer, acute lymphoblastic leukemia (ALL) and acute myeloid leukemia (AML), using gene expression measurements. \n",
    "\n",
    "The data set is provided in the file `dataset_hw2.csv`. \n",
    "* Rows in the file:  tumor tissue samples from a patient with one of the two forms of leukemia. \n",
    "* The first column contains the cancer type, with 0 indicating the ALL class and 1 indicating the AML class. \n",
    "* Columns 2-7130 contain expression levels of 7129 genes recorded from each tissue sample. \n",
    "\n",
    "##### Goals of this part\n",
    "Use logistic regression to build a classification model for this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.1 [6 pts]: Data Exploration\n",
    "\n",
    "### Part 2.1.1 [2 pts]:  Pre-processing\n",
    "* Split  the observations into an approximate 50-50 train-test split.  Below is some code to do this for you (we want to make sure everyone has the same splits).\n",
    "```python\n",
    "np.random.seed(9001)\n",
    "df = pd.read_csv('dataset_hw2.csv')\n",
    "msk = np.random.rand(len(df)) < 0.5\n",
    "data_train = df[msk]\n",
    "data_test = df[~msk]\n",
    "```\n",
    "* Take a peak at your training set: you should notice the severe differences in the measurements from one gene to the next (some are negative, some hover around zero, and some are well into the thousands).  To account for these differences in scale and variability, normalize each predictor to vary between 0 and 1.\n",
    "\n",
    "#### Deliverables\n",
    "Your code should be contained in a Jupyter notebook cell. An appropriate level of comments is necessary. Your code should run.\n",
    "\n",
    "#### Required Outputs\n",
    "None for this section.\n",
    "\n",
    "### Part 2.1.2 [2 pts]:  Heat Map\n",
    "A convenient tool to visualize the gene expression data is a heat map. \n",
    "* Arrange the rows of the training set so that the *AML* rows are grouped together and the *ALL* rows are together. * * Generate a heat map of the data with expression values from the following genes:\n",
    "  - `D49818_at`\n",
    "  - `M23161_at`\n",
    "  - `hum_alu_at`\n",
    "  - `AFFX-PheX-5_at`\n",
    "  - `M15990_at`\n",
    "\n",
    "  **Hint:** Take a look at the `seaborn` `heatmap` method:  [`seaborn` heat map](https://seaborn.pydata.org/generated/seaborn.heatmap.html) and [seaborn-heatmap-using-pandas-dataframe](https://stackoverflow.com/questions/37790429/seaborn-heatmap-using-pandas-dataframe).\n",
    "\n",
    "#### Deliverables\n",
    "Your code should be contained in a Jupyter notebook cell. An appropriate level of comments is necessary. Your code should run and output the required outputs described below.\n",
    "\n",
    "#### Required Outputs\n",
    "1. A properly labeled heat map.\n",
    "  * Properly labeled means $x$ and $y$ axes are labeled and a colorbar is included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1.3 [2 pts]:  Discussion Questions\n",
    "\n",
    "1. [1 pt] Notice that the results training set contains more predictors than observations. Do you foresee a problem in fitting a classification model to such a data set?\n",
    "\n",
    "  *Your answer here.*\n",
    "\n",
    "2. [1 pt] By observing the heat map, comment on which of these genes are useful in discriminating between the two classes.\n",
    "\n",
    "  *Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.2 [16 pts]:  Linear Regression vs. Logistic Regression\n",
    "In this part, you will analyze the differences between using linear regression and logistic regression for classification. For this part, you will work with a single gene predictor: `D29963_at`.\n",
    "\n",
    "### 2.2.1 [2 pts]:  Linear Regression\n",
    "* Fit a simple linear regression model to the training set using the single gene predictor `D29963_at`.\n",
    "\n",
    "#### Deliverables\n",
    "Your code should be contained in a Jupyter notebook cell. An appropriate level of comments is necessary. Your code should run and output the required outputs described below.\n",
    "\n",
    "#### Required Outputs\n",
    "* Print the prediction from the linear regression model, $\\widehat{y}_{\\textrm{test}}$\n",
    "\n",
    "### 2.2.2 [2 pts]:  Convert to Classification Model\n",
    "The fitted linear regression model can be converted to a classification model (i.e. a model that predicts one of two binary labels 0 or 1) by classifying patients with predicted score greater than 0.5 into the `ALL` type (class 1), and the others into the `AML` type (class 0).\n",
    "\n",
    "* Evaluate the classification accuracy ($1$ - misclassification rate) of the obtained classification model on both the training and test sets.\n",
    "\n",
    "#### Deliverables\n",
    "Your code should be contained in a Jupyter notebook cell. An appropriate level of comments is necessary. Your code should run and output the required outputs described below.\n",
    "\n",
    "#### Required Outputs\n",
    "* Print the train accuracy and test accuracy.  For example:\n",
    "```python\n",
    "Train accuracy:  0.78125\n",
    "Test accuracy:  0.731707317073\n",
    "```\n",
    "\n",
    "### 2.2.3 [2 pts]:  Logistic Regression\n",
    "Fit a simple logistic regression model to the training set.\n",
    "**Hint:**:  Remember, you need to set the regularization parameter for `sklearn`'s logistic regression function to be a very large value in order not to regularize (use 'C=100000').\n",
    "\n",
    "#### Deliverables\n",
    "Your code should be contained in a Jupyter notebook cell. An appropriate level of comments is necessary. Your code should run and output the required outputs described below.\n",
    "\n",
    "#### Required Outputs\n",
    "* Print the train and test accuracy.  For example:\n",
    "```python\n",
    "Train accuracy:  0.75\n",
    "Test accuracy:  0.80487804878\n",
    "```\n",
    "\n",
    "### 2.2.4 [5 pts]:  Visualize Results\n",
    "* Plot the quantitative output from linear regression model and the probabilistic output from the logistic regression model (on the training set points) as a function of the gene predictor.\n",
    "* Display the true binary response for the training set points in the same plot.\n",
    "\n",
    "#### Deliverables\n",
    "Your code should be contained in a Jupyter notebook cell. An appropriate level of comments is necessary. Your code should run and output the required outputs described below.\n",
    "\n",
    "#### Required Outputs\n",
    "1. One figure containing three plots side-by-side:\n",
    "  a. The left plot should be the `OLS` prediction\n",
    "  b. The middle plot should be the logistic prediction\n",
    "  c. The right plot should be the true binary response\n",
    "2. Make the same plot for the test set points.\n",
    "3. Be sure to properly annotate your figures (label $x$ and $y$ axes, put a title, put a legend)\n",
    "\n",
    "**Hint**:  For side-by-side plots, consider using the construct:\n",
    "```python\n",
    "fig, ax = plt.subplots(1,3, figsize=(10,6))\n",
    "ax[0].plot(...) # left plot\n",
    "ax[1].plot(...) # middle plot\n",
    "ax[2].plot(...) # right plot\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 [3 pts]:  Discussion Questions\n",
    "1. [1 pt] We could interpret the scores predicted by regression model (part $2.2.1$) interpreted for a patient as an estimate of the probability that the patient has the `ALL` type cancer (class 1). Is there a problem with this interpretation?\n",
    "\n",
    "  *Your answer here.*\n",
    "\n",
    "2. [1 pt] How does the training and test classification accuracy of the logistic regression model compare with the linear regression model?\n",
    "\n",
    "  *Your answer here.*\n",
    "\n",
    "3. [1 pt] Based on these plots in part $2.2.4$, does one of the models appear better suited for binary classification than the other? Explain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
